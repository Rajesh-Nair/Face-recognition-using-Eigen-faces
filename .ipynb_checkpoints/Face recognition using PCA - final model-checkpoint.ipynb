{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Face-recognition---training-and-prediction\" data-toc-modified-id=\"Face-recognition---training-and-prediction-1\">Face recognition - training and prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1.1\">Load data</a></span></li></ul></li><li><span><a href=\"#Face-recognition-model\" data-toc-modified-id=\"Face-recognition-model-2\">Face recognition model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stratified-train-test-split\" data-toc-modified-id=\"Stratified-train-test-split-2.1\">Stratified train-test split</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition using PCA\n",
    "\n",
    "Lets pick from where we left. We've extracted about 200 Principal components from the set of images with 4096 pixels/features however we don't need all the 200 components to recognize a face. From the previous notebook we could recognize some random faces with as low as 60 components accurately however we need a number that would maximize the prediction accuracy. Lets create few models and train them to predict the faces. We will then evaluate the train and test accuracy of each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library / packages\n",
    "\n",
    "# For numerical operation\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for tabular manipulations\n",
    "import pandas as pd\n",
    "\n",
    "# For plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# K-Nearest Neighbour\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 64, 64), (400,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the faces\n",
    "faces_image = np.load('Faces\\\\olivetti_faces.npy')\n",
    "faces_target = np.load('Faces\\\\olivetti_faces_target.npy')\n",
    "\n",
    "# Find the dimension\n",
    "faces_image.shape, faces_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_recognize :\n",
    "    \n",
    "    def __init__(self,n_components=100,model = 'SVCRBF', mirror_face = 'Y', scaler='MinMax') :\n",
    "        \n",
    "        assert(isinstance(n_components,int))\n",
    "        self.n = n_components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        \n",
    "        assert(model in ['SVCRBF', 'KNN1EU'])\n",
    "        self.model = model\n",
    "        if model == 'SVCRBF' :\n",
    "            self.mod = SVC(kernel='rbf', gamma='auto')\n",
    "        elif model == 'KNN1EU' :\n",
    "            self.mod = KNeighborsClassifier(n_neighbors=1)\n",
    "        \n",
    "        assert(mirror_face in ['Y', 'N'])\n",
    "        self.mirror = mirror_face\n",
    "        \n",
    "        assert(scaler in ['MinMax', 'Std', None])\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        str = 'Face_recognize(n_components={}, model = {}, mirror_face = {}, scaler={})'.\\\n",
    "        format(self.n,self.model,self.mirror,self.scaler)\n",
    "        print(str)\n",
    "        \n",
    "        \n",
    "    def fit(self,X,Y) :\n",
    "                           \n",
    "        # Add Mirror faces\n",
    "        Transform_mat = np.flip(np.diagflat(np.ones(faceimg_train.shape[1])),1)\n",
    "        \n",
    "        def Mirror_face(X) :\n",
    "            return(np.dot(X,Transform_mat))\n",
    "\n",
    "        for i, face in enumerate(X) :\n",
    "            New_face = Mirror_face(face).reshape(1,face.shape[0], face.shape[1])\n",
    "            X = np.append(X,New_face, axis=0)\n",
    "            Y = np.append(Y,Y[i])\n",
    "            \n",
    "        # Normalize\n",
    "        X = X.reshape(X.shape[0],-1)\n",
    "        if self.scaler == 'MinMax' :\n",
    "            XS = X/(np.max(X,axis=1,keepdims=True)-np.min(X,axis=1,keepdims=True))\n",
    "        elif self.scaler == 'Std' :\n",
    "            XS = X/np.std(X,axis=1,keepdims=True)\n",
    "        elif self.scaler == None :\n",
    "            XS = X\n",
    "            \n",
    "                    \n",
    "        # Find PCA        \n",
    "        Weights_train = self.pca.fit_transform(XS)\n",
    "        self.Mean_face = self.pca.mean_\n",
    "        self.Eigen_vec = self.pca.components_\n",
    "                \n",
    "        \n",
    "        # Find mean weights by class\n",
    "        self.W_mean = pd.concat([pd.DataFrame(Y, columns = ['Label']), pd.DataFrame(Weights_train)], axis = 1)\\\n",
    "                                .groupby(['Label']).mean().values        \n",
    "        out = self._eu_classdist(Weights_train, self.W_mean)\n",
    "        \n",
    "        # Maximum allowed distance from vector\n",
    "        self.max_dist = np.max(out[:,1])\n",
    "        \n",
    "        # Maximum allowed error in regenerated face\n",
    "        self.max_err = 1.8*np.max(self._face_diff(XS, self.Mean_face, Weights_train, self.Eigen_vec),axis=0)\n",
    "        \n",
    "        # Train the final model\n",
    "        self.mod.fit(Weights_train,Y)\n",
    "            \n",
    "            \n",
    "    # Find euclidean distance from each class\n",
    "    def _eu_classdist(self,X,Mean) :\n",
    "        distance = np.empty([X.shape[0], Mean.shape[0]])   \n",
    "        for i, mean in enumerate(Mean) :            \n",
    "            distance[:,i] = np.linalg.norm((X-mean.reshape(1,-1)), axis=1)  \n",
    "        output = np.vstack((np.argmin(distance, axis = 1),np.min(distance, axis = 1))).T\n",
    "        return(output)\n",
    "    \n",
    "    # Find error of regenerated face\n",
    "    def _face_diff(self,Face,Mean_face,Weights,Eigen_vec) :\n",
    "        Regen_face = Mean_face.reshape(1,-1) + np.dot(Weights,Eigen_vec)\n",
    "        return(np.linalg.norm((Face.reshape(Regen_face.shape[0],Regen_face.shape[1])-Regen_face),axis=1))\n",
    "    \n",
    "            \n",
    "    def predict(self,X) :\n",
    "        \n",
    "        # Normalize\n",
    "        X = X.reshape(X.shape[0],-1)\n",
    "        if self.scaler == 'MinMax' :\n",
    "            XS = X/(np.max(X,axis=1,keepdims=True)-np.min(X,axis=1,keepdims=True))\n",
    "        elif self.scaler == 'Std' :\n",
    "            XS = X/np.std(X,axis=1,keepdims=True)\n",
    "        elif self.scaler == None :\n",
    "            XS = X\n",
    "\n",
    "        \n",
    "        # PCA\n",
    "        Weights_test = self.pca.transform(XS)\n",
    "        \n",
    "        \n",
    "        # Weight check\n",
    "        out = self._eu_classdist(Weights_test, self.W_mean)\n",
    "                    \n",
    "        \n",
    "        # Error check\n",
    "        err = self._face_diff(XS, self.Mean_face, Weights_test, self.Eigen_vec)\n",
    "                        \n",
    "        # Model prediction\n",
    "        Prediction = self.mod.predict(Weights_test)\n",
    "        \n",
    "        # Prediction after check\n",
    "        return(np.array([np.nan if out[i,1] > self.max_dist or err[i] > self.max_err else p \\\n",
    "                for i, p in enumerate(Prediction)]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified train-test split\n",
    "The split should be stratified for accurate test results. Stratified split ensures that data from each class/category of dataset are distributed between train and test based on the size defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test images\n",
    "faceimg_train, faceimg_test, facetrg_train, facetrg_test = train_test_split(faces_image, faces_target,\\\n",
    "                                                    stratify = faces_target, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face_recognize(n_components=80, model = SVCRBF, mirror_face = Y, scaler=MinMax)\n"
     ]
    }
   ],
   "source": [
    "# Initiate the model\n",
    "model = Face_recognize(n_components=80, scaler = 'MinMax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(faceimg_train,facetrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data prediction\n",
    "Train_pred = model.predict(faceimg_train)\n",
    "\n",
    "# Test data prediction\n",
    "Test_pred = model.predict(faceimg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "np.sum(facetrg_train == Train_pred)*100/facetrg_train.shape[0]\n",
    "#np.sum(Y_train_pred.reshape(-1) == Y_train.reshape(-1))*100/Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.83333333333333"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(facetrg_test == Test_pred)*100/facetrg_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4., 38.,  2.,  8., 37., 15., 30., 23., 35., 34., nan,  3., 12.,\n",
       "       32., 17., 38.,  6.,  6., 22., 16., 26., 10., 35., 29., 30., 32.,\n",
       "       36., 21., 31., 33.,  2., 26.,  3., 35., 20., 32.,  4., 27., 34.,\n",
       "       23., 19., 36.,  4., 20., 12.,  7., 15., 20., 21.,  5.,  9.,  0.,\n",
       "       11., 15., 34., 38.,  0., 28., 33., 14., 21., 17., 14., 24., 27.,\n",
       "       17.,  8., 13., 26., 27., 19.,  1.,  1., 39., 22., 18., 38., 25.,\n",
       "       22., 36., 11., 25.,  2., 11.,  0.,  1.,  8., 31., 30.,  9., 37.,\n",
       "       24., 33.,  9., 16., 18., 13., 28., 28., 14., 14., 39., 24.,  5.,\n",
       "       37.,  6., 31., 18.,  7., 29., 23.,  4., 25.,  3., 19.,  5., 39.,\n",
       "       13., 16., 35.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facetrg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
