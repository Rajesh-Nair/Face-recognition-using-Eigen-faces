{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Face-recognition-using-PCA\" data-toc-modified-id=\"Face-recognition-using-PCA-1\">Face recognition using PCA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1.1\">Load data</a></span></li><li><span><a href=\"#Face-recognition-model\" data-toc-modified-id=\"Face-recognition-model-1.2\">Face recognition model</a></span></li><li><span><a href=\"#Hold-out-n-fold-cross-validation\" data-toc-modified-id=\"Hold-out-n-fold-cross-validation-1.3\">Hold out n fold cross validation</a></span></li><li><span><a href=\"#Stratified-train-test-split\" data-toc-modified-id=\"Stratified-train-test-split-1.4\">Stratified train-test split</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition using PCA\n",
    "\n",
    "Our final model would be based on the analysis we have done in the previous notebook. We would pick the best part of different models we tried and create a new model. This model would use PCA and SVC but also look at ways to find the faces that are not part of the training distribution for e.g. random noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library / packages\n",
    "\n",
    "# For numerical operation\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for tabular manipulations\n",
    "import pandas as pd\n",
    "\n",
    "# For plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Stratified n-fold cv\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# K-Nearest Neighbour\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 64, 64), (400,))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the faces\n",
    "faces_image = np.load('Faces\\\\olivetti_faces.npy')\n",
    "faces_target = np.load('Faces\\\\olivetti_faces_target.npy')\n",
    "\n",
    "# Find the dimension\n",
    "faces_image.shape, faces_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face recognition model\n",
    "class Face_recognize :\n",
    "    \n",
    "    def __init__(self,n_components=100,model = 'SVCRBF', mirror_face = 'Y', scaler='MinMax', \\\n",
    "                 k_w=1, k_e=1.8, print_info=True) :\n",
    "        \n",
    "        assert(isinstance(n_components,int))\n",
    "        self.n = n_components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        \n",
    "        assert(model in ['SVCRBF', 'KNN1EU'])\n",
    "        self.model = model\n",
    "        if model == 'SVCRBF' :\n",
    "            self.mod = SVC(kernel='rbf', gamma='auto')\n",
    "        elif model == 'KNN1EU' :\n",
    "            self.mod = KNeighborsClassifier(n_neighbors=1)\n",
    "        \n",
    "        assert(mirror_face in ['Y', 'N'])\n",
    "        self.mirror = mirror_face\n",
    "        \n",
    "        assert(scaler in ['MinMax', 'Std', None])\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        assert(isinstance(k_w, (int, float)))        \n",
    "        self.k_w = k_w\n",
    "        \n",
    "        assert(isinstance(k_e, (int, float)))\n",
    "        self.k_e = k_e\n",
    "        \n",
    "        str = 'Face_recognize(n_components={}, model = {}, mirror_face = {}, scaler={}, k_w={}, k_e={}, print_info={})'.\\\n",
    "        format(self.n,self.model,self.mirror,self.scaler,self.k_w,self.k_e,print_info)\n",
    "        if print_info :\n",
    "            print(str)\n",
    "        \n",
    "        \n",
    "    def fit(self,X,Y) :\n",
    "                           \n",
    "        # Add Mirror faces\n",
    "        Transform_mat = np.flip(np.diagflat(np.ones(X.shape[1])),1)\n",
    "        \n",
    "        def Mirror_face(X) :\n",
    "            return(np.dot(X,Transform_mat))\n",
    "\n",
    "        for i, face in enumerate(X) :\n",
    "            New_face = Mirror_face(face).reshape(1,face.shape[0], face.shape[1])\n",
    "            X = np.append(X,New_face, axis=0)\n",
    "            Y = np.append(Y,Y[i])\n",
    "            \n",
    "        # Normalize\n",
    "        X = X.reshape(X.shape[0],-1)\n",
    "        if self.scaler == 'MinMax' :\n",
    "            XS = X/(np.max(X,axis=1,keepdims=True)-np.min(X,axis=1,keepdims=True))\n",
    "        elif self.scaler == 'Std' :\n",
    "            XS = X/np.std(X,axis=1,keepdims=True)\n",
    "        elif self.scaler == None :\n",
    "            XS = X\n",
    "            \n",
    "                    \n",
    "        # Find PCA        \n",
    "        Weights_train = self.pca.fit_transform(XS)\n",
    "        self.Mean_face = self.pca.mean_\n",
    "        self.Eigen_vec = self.pca.components_\n",
    "                \n",
    "        \n",
    "        # Find mean weights by class\n",
    "        self.W_mean = pd.concat([pd.DataFrame(Y, columns = ['Label']), pd.DataFrame(Weights_train)], axis = 1)\\\n",
    "                                .groupby(['Label']).mean().values        \n",
    "        out = self._eu_classdist(Weights_train, self.W_mean)\n",
    "        \n",
    "        # Maximum allowed distance from vector\n",
    "        self.max_dist = self.k_w*np.max(out[:,1])\n",
    "        \n",
    "        # Maximum allowed error in regenerated face\n",
    "        self.max_err = self.k_e*np.max(self._face_diff(XS, self.Mean_face, Weights_train, self.Eigen_vec),axis=0)\n",
    "        \n",
    "        # Train the final model\n",
    "        self.mod.fit(Weights_train,Y)\n",
    "            \n",
    "            \n",
    "    # Find euclidean distance from each class\n",
    "    def _eu_classdist(self,X,Mean) :\n",
    "        distance = np.empty([X.shape[0], Mean.shape[0]])   \n",
    "        for i, mean in enumerate(Mean) :            \n",
    "            distance[:,i] = np.linalg.norm((X-mean.reshape(1,-1)), axis=1)  \n",
    "        output = np.vstack((np.argmin(distance, axis = 1),np.min(distance, axis = 1))).T\n",
    "        return(output)\n",
    "    \n",
    "    # Find error of regenerated face\n",
    "    def _face_diff(self,Face,Mean_face,Weights,Eigen_vec) :\n",
    "        Regen_face = Mean_face.reshape(1,-1) + np.dot(Weights,Eigen_vec)\n",
    "        return(np.linalg.norm((Face.reshape(Regen_face.shape[0],Regen_face.shape[1])-Regen_face),axis=1))\n",
    "    \n",
    "            \n",
    "    def predict(self,X) :\n",
    "        \n",
    "        # Normalize\n",
    "        X = X.reshape(X.shape[0],-1)\n",
    "        if self.scaler == 'MinMax' :\n",
    "            XS = X/(np.max(X,axis=1,keepdims=True)-np.min(X,axis=1,keepdims=True))\n",
    "        elif self.scaler == 'Std' :\n",
    "            XS = X/np.std(X,axis=1,keepdims=True)\n",
    "        elif self.scaler == None :\n",
    "            XS = X\n",
    "\n",
    "        \n",
    "        # PCA\n",
    "        Weights_test = self.pca.transform(XS)\n",
    "        \n",
    "        \n",
    "        # Weight check\n",
    "        out = self._eu_classdist(Weights_test, self.W_mean)\n",
    "                    \n",
    "        \n",
    "        # Error check\n",
    "        err = self._face_diff(XS, self.Mean_face, Weights_test, self.Eigen_vec)\n",
    "                        \n",
    "        # Model prediction\n",
    "        Prediction = self.mod.predict(Weights_test)\n",
    "        \n",
    "        # Prediction after check\n",
    "        return(np.array([np.nan if out[i,1] > self.max_dist or err[i] > self.max_err else p \\\n",
    "                for i, p in enumerate(Prediction)]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold out n fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "def cross_val(model,X,Y,cv=5) :\n",
    "    \n",
    "    # Kfold CV\n",
    "    k_fold = StratifiedKFold(n_splits = 5, random_state=123)\n",
    "\n",
    "    Train_score = np.array([])\n",
    "    Test_score = np.array([])\n",
    "    for i,j in k_fold.split(X,Y) :\n",
    "        model.fit(X[i,:,:],Y[i])\n",
    "        Train_pred = model.predict(X[i,:,:])\n",
    "        Test_pred = model.predict(X[j,:,:])\n",
    "        score = np.array([1*(Y[i]==Train_pred)])\n",
    "        Train_score = np.append(Train_score, np.mean(score))\n",
    "        score = np.array([1*(Y[j]==Test_pred)])\n",
    "        Test_score = np.append(Test_score, np.mean(score))\n",
    "\n",
    "    return(Train_score, Test_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face_recognize(n_components=80, model = SVCRBF, mirror_face = Y, scaler=MinMax, k_w=1, k_e=1.8, print_info=True)\n",
      "Train error 99.94 % +/- 0.13\n",
      "Test error 95.5 % +/- 3.41\n"
     ]
    }
   ],
   "source": [
    "# Number of components = 80 - Cross validation results\n",
    "model = Face_recognize(n_components=80, scaler = 'MinMax',print_info=True)\n",
    "Train_score, Test_score = cross_val(model, faces_image, faces_target, cv=5)\n",
    "\n",
    "print('Train error {} % +/- {}'.format(np.round(100*np.mean(Train_score),2),np.round(100*np.std(Train_score),2)))\n",
    "print('Test error {} % +/- {}'.format(np.round(100*np.mean(Test_score),2),np.round(100*np.std(Test_score),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test images\n",
    "faceimg_train, faceimg_test, facetrg_train, facetrg_test = train_test_split(faces_image, faces_target,\\\n",
    "                                                    stratify = faces_target, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random noise\n",
    "Random_face = np.random.random([120,64,64])\n",
    "Z1 = Random_face[0:60,:,:]/10\n",
    "Z2 = Random_face[60:120,:,:]\n",
    "Z1Y = np.ones(Z1.shape[0])*np.nan\n",
    "Z2Y = np.ones(Z2.shape[0])*np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the daa in test daya\n",
    "X_test = np.concatenate((faceimg_test,Z1,Z2),axis=0)\n",
    "Y_test = np.concatenate((facetrg_test,Z1Y,Z2Y),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face_recognize(n_components=80, model = SVCRBF, mirror_face = Y, scaler=MinMax, k_w=1, k_e=1.8, print_info=True)\n"
     ]
    }
   ],
   "source": [
    "# Model - Train\n",
    "model = Face_recognize(n_components=80, scaler = 'MinMax',print_info=True)\n",
    "model.fit(faceimg_train,facetrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train prediction and accuracy\n",
    "Train_pred = model.predict(faceimg_train)\n",
    "np.round(100*np.mean(np.array([(a==b)|(np.isnan(a) & np.isnan(b)) for a, b in zip(Train_pred, facetrg_train)])),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test prediction and accuracy\n",
    "Test_pred = model.predict(X_test)\n",
    "np.round(100*np.mean(np.array([(a==b)|(np.isnan(a) & np.isnan(b)) for a, b in zip(Test_pred, Y_test)])),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would see the model predicts seen and unseen images upto 97.5% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
